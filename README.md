# ðŸ“˜ Scientific Concept Understanding via Fine-tuning LLM

## ðŸ§  Project Overview

This project fine-tunes the `google/gemma-2b-it` or `gemma-2b` model for **scientific concept understanding**, targeting multi-format supervised tasks like:

- ðŸ§¾ **Context-based understanding**
- ðŸ“– **Definition-based classification**
- ðŸ§ª **Example-based prediction**

All training is LoRA-compatible and Hugging Face Transformers-based. The model can be deployed on local CPU/GPU or remote cloud servers for inference and evaluation.

## ðŸ“Š Supported Tasks

| Task Type         | Input Format                              | Output (Label/Text) |
|-------------------|-------------------------------------------|----------------------|
| Context-based     | Scientific context paragraph               | Classification label |
| Definition-based  | Technical definition of a scientific term | Classification label |
| Example-based     | Example description sentence              | Classification label |

## ðŸ—‚ï¸ Dataset Format (JSON)

Each data sample should be formatted as follows:

```json
{
  "input": "This is a scientific context or definition...",
  "label": "correct/incorrect"
}
```

## ðŸ’» Environment Setup

```bash
git clone https://github.com/yourname/sci-finetune.git
cd sci-finetune
conda create -n sci python=3.10 -y
conda activate sci
pip install -r requirements.txt
```

### ðŸ”§ requirements.txt

```txt
torch>=2.1.0
transformers>=4.40.0
datasets
scikit-learn
peft
accelerate
huggingface_hub
sentencepiece
```

## ðŸš€ Training

```bash
python train.py \
    --model_name_or_path google/gemma-2b-it \
    --train_file data/context_train.json \
    --validation_file data/context_val.json \
    --output_dir checkpoints/context_lora \
    --use_lora
```

## ðŸ§ª Evaluation Report Generation

```bash
python generate_eval_report.py \
    --model_path checkpoints/context_lora \
    --data_file data/context_val.json
```

## ðŸ¤– Inference Script

```bash
python predict.py \
    --model_path checkpoints/context_lora \
    --use_lora \
    --prompt "Define quantum entanglement."
```

## ðŸ§© File Structure

```
sci_fine_tuning/
â”œâ”€â”€ train.py
â”œâ”€â”€ predict.py
â”œâ”€â”€ generate_eval_report.py
â”œâ”€â”€ metrics.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ context_train.json
â”‚   â””â”€â”€ context_val.json
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ context_lora/
```

## ðŸ”’ Hugging Face Auth

```bash
huggingface-cli login
```

## ðŸ§¯ Troubleshooting

| Error | Solution |
|-------|----------|
| `No module named sklearn` | `pip install scikit-learn` |
| `Token not accepted` | Use `huggingface-cli login` or set `HF_TOKEN` manually |
| `Unexpected argument: evaluation_strategy` | Downgrade Transformers or use correct class (`TrainingArguments`) |
| `OSError: not a valid model identifier` | Use local path instead of HF repo URL |
| `--load_best_model_at_end` error | Ensure `evaluation_strategy == save_strategy` |

## ðŸ“ Next Steps

- [ ] Add multi-GPU support
- [ ] ONNX / TensorRT conversion
- [ ] Web UI deployment
- [ ] Batch API

## ðŸ‘©â€ðŸ’» Maintainer

> Author: [@kiana0512](https://github.com/kiana0512)  
> Email: contact@yourdomain.com