# sci_fine_tuning/evaluate_generation.py

import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
from tqdm import tqdm
from prompt_template import build_prompt
from metrics import compute_metrics  # ä½¿ç”¨ä½ åŸæ¥çš„ metrics.py
from collections import Counter


def generate_prediction(model, tokenizer, prompt, max_new_tokens=8):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.eos_token_id
        )
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    generated = decoded[len(prompt):].strip()
    prediction = generated.split()[0].strip("ï¼š:ï¼Œ,ã€‚ \n")
    return prediction


def main():
    base_model_path = "./gemma_local"       # âœ… åŸå§‹åŸºåº§æ¨¡å‹
    lora_path = "checkpoints/context_lora/checkpoint-570"       # âœ… LoRA å‚æ•°è·¯å¾„
    data_path = "./processed/context_split"        # âœ… ä½ çš„æ•°æ®è·¯å¾„

    # åŠ è½½æ•°æ®é›†ï¼ˆtrain_test_split åï¼‰
    dataset = load_from_disk(data_path)["validation"]  # ä½¿ç”¨ test åˆ†æ”¯åšéªŒè¯
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map="auto", torch_dtype=torch.float16)
    model = PeftModel.from_pretrained(model, lora_path)  # âœ… åº”ç”¨ LoRA å‚æ•°

    pred_labels = []
    true_labels = []

    print("ğŸ” æ­£åœ¨ç”Ÿæˆé¢„æµ‹...")
    for example in tqdm(dataset):
        prompt = build_prompt(example)
        pred = generate_prediction(model, tokenizer, prompt)
        pred_labels.append(pred)
        true_labels.append(example["output"])  # âœ… output æ˜¯çœŸå®æ ‡ç­¾

    # è¾“å‡ºå‰10å¯¹æ¯”
    print("\nğŸ‘€ å‰10ä¸ªé¢„æµ‹ vs å®é™…æ ‡ç­¾ï¼š")
    for i in range(min(10, len(pred_labels))):
        print(f"{i + 1}. Pred: {pred_labels[i]} | Label: {true_labels[i]}")

    print("\nğŸ“Š é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒï¼š", Counter(pred_labels))
    print("ğŸ“Š çœŸå®æ ‡ç­¾åˆ†å¸ƒï¼š", Counter(true_labels))

    print("\nâœ… éªŒè¯é›†è¯„ä¼°ç»“æœï¼š")
    results = compute_metrics((pred_labels, true_labels))
    for k, v in results.items():
        print(f"{k}: {v:.4f}")


if __name__ == "__main__":
    main()
